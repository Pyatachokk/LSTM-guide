{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1\n",
    "batch_size = 1\n",
    "sample_size = 1\n",
    "sequence_length = 1\n",
    "learning_rate = 0.01\n",
    "hidden_layer_size = 10\n",
    "\n",
    "\n",
    "def norm_array(array):\n",
    "    mean = np.mean(array)\n",
    "    stddev = np.sqrt(np.sum(np.square(array - mean)) / len(array - 1))\n",
    "    normalized_array = ((array - mean) / stddev)\n",
    "    return normalized_array, mean.reshape(batch_size), stddev.reshape(batch_size)\n",
    "\n",
    "def norm_test(array, mean, stddev):\n",
    "    return (array - mean) / stddev\n",
    "\n",
    "def test_decoder(array, mean, stddev):\n",
    "    return (array * stddev) + mean\n",
    "\n",
    "def get_linear_layer(vector):\n",
    "    return(tf.matmul(vector, Wl) + bl)\n",
    "def train_test_split(X, y, train_size):\n",
    "    X_train = X[:int(len(X) // (1 / train_size))]\n",
    "    y_train = y[:int(len(y) // (1 / train_size))]\n",
    "    X_test = X[int(len(X) // (1 / train_size)):]\n",
    "    y_test = y[int(len(y) // (1 / train_size)):]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def data_labels(data, sample_size, batch_size):\n",
    "    labels = data.iloc[:sample_size, 2:]\n",
    "    data = data.iloc[:sample_size, 1:]\n",
    "    indeces = np.random.choice(len(data), size=batch_size, replace=False)\n",
    "    batch_x = np.array(data.loc[indeces[0]][data.loc[indeces[0]][:] > 0][:-1])\n",
    "    batch_y = np.array(labels.loc[indeces[0]][labels.loc[indeces[0]][:] > 0])\n",
    "    return (batch_x, batch_y)\n",
    "\n",
    "\n",
    "def cell_list(num_LSTM_layers):\n",
    "    cell_list = []\n",
    "    for i in range(num_LSTM_layers):\n",
    "        cell_list.append(tf.contrib.rnn.BasicLSTMCell(hidden_layer_size, forget_bias=1.0))\n",
    "    return cell_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_monthly = pd.read_csv(r\"C:\\M4datasets\\Monthly-train.csv\")\n",
    "data_month, labels_month = data_labels(data_monthly, sample_size = 1, batch_size = 1)\n",
    "data_month = data_month[:90]\n",
    "labels_month = labels_month[:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = [1]\n",
    "num_hidden = np.arange(25, 250, 2)\n",
    "param_list_2 = [(x, y) for x in num_layers for y in num_hidden]\n",
    "\n",
    "# num_layers = [1, 2]\n",
    "# num_hidden = np.arange(25, 250, 2)\n",
    "# param_list_2 = [(x, y) for x in num_layers for y in num_hidden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_iter = 912\n",
    "stop_iters = []\n",
    "stop_params = []\n",
    "for param in param_list_2:\n",
    "    num_LSTM_layers = param[0]\n",
    "    hidden_layer_size = param[1]\n",
    "    tf.reset_default_graph()\n",
    "    with tf.name_scope(\"Training_data\"):\n",
    "        inputs = tf.placeholder(tf.float32,shape=[batch_size, None, sequence_length], name='inputs')\n",
    "        y = tf.placeholder(tf.float32, shape=[batch_size], name='inputs')\n",
    "\n",
    "    with tf.name_scope('Linear_weights_and_biases'):\n",
    "        Wl = tf.Variable(tf.truncated_normal([hidden_layer_size, num_classes], mean = 0, stddev = 0.01))\n",
    "        bl = tf.Variable(tf.truncated_normal([num_classes], mean = 0, stddev = 0.1))\n",
    "\n",
    "    with tf.name_scope('Means_and_standard_deviations'):\n",
    "        mean = tf.placeholder(tf.float32, shape = [batch_size])\n",
    "        stddev = tf.placeholder(tf.float32, shape = [batch_size])\n",
    "\n",
    "    with tf.variable_scope('LSTM_cell'):  \n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells=cell_list(num_LSTM_layers), state_is_tuple=True)\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "\n",
    "\n",
    "    with tf.variable_scope('Batch_outputs'):\n",
    "        final_outputs = tf.placeholder\n",
    "\n",
    "    linear_output = get_linear_layer(outputs[0])\n",
    "    final_output = test_decoder(linear_output, mean, stddev)[-1]\n",
    "    mse = tf.reduce_mean(tf.squared_difference(final_output, y[0]))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(mse)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    mse_list = []\n",
    "    for i in range(2000):\n",
    "        batch_x, batch_y = data_day, labels_day\n",
    "        data_train, data_test, labels_train, labels_test = train_test_split(batch_x, batch_y, train_size = 0.8)\n",
    "\n",
    "        normed_data_train, mean_batch, stddev_batch = norm_array(data_train)\n",
    "\n",
    "        normed_data_train = normed_data_train.reshape(batch_size, len(data_train), sequence_length)\n",
    "\n",
    "        label = labels_train[-1].reshape(batch_size)\n",
    "        sess.run(train_step, feed_dict = {inputs : normed_data_train,\\\n",
    "                                                y : label,  mean : mean_batch, stddev : stddev_batch})\n",
    "        mse_batch = sess.run(mse, feed_dict = {inputs : normed_data_train,\\\n",
    "                                                y : label,  mean : mean_batch, stddev : stddev_batch})\n",
    "        for j in range(len(data_test)):\n",
    "            data_train =  np.append(data_train, data_test[j])\n",
    "            labels_train = np.append(labels_train, labels_test[j])\n",
    "            normed_data_train, mean_batch, stddev_batch = norm_array(data_train)\n",
    "            normed_data_train = normed_data_train.reshape(batch_size, len(data_train), sequence_length)\n",
    "\n",
    "            label = labels_train[-1].reshape(batch_size)\n",
    "            sess.run(train_step, feed_dict = {inputs : normed_data_train, y : label, mean : mean_batch, stddev : stddev_batch})\n",
    "            mse_batch += sess.run(mse, feed_dict = {inputs : normed_data_train,\\\n",
    "                                                y : label,  mean : mean_batch, stddev : stddev_batch})\n",
    "        mse_batch = mse_batch / len(data_test)\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        if mse_batch <= 1 and i < stop_iter: #For daily data important to change mse_batch <= 0.01\n",
    "            stop_iter = i\n",
    "            stop_iters.append(stop_iter)\n",
    "            stop_params.append(param)\n",
    "            print(stop_iter, param)\n",
    "            break\n",
    "        if i >= stop_iter:\n",
    "            break\n",
    "    sess.close()\n",
    "    print(param, stop_iter)\n",
    "\n",
    "print(stop_iters)\n",
    "print(stop_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
